{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating A GAN-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Philipp\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helperfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "    initial = tf.glorot_uniform_initializer()\n",
    "    return tf.Variable(initial(shape))\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.zeros.initializer()\n",
    "    return tf.Variable(initial(shape))\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "def conv2d(x, W):\n",
    "    return tf.nn.conv2d(x, filter=W, strides=[1, 1, 1, 1], padding='VALID')\n",
    "\n",
    "def conv2d_transposed(x, filters): \n",
    "    return tf.layers.conv2d_transpose(inputs=x, filters=filters, kernel_size=[2, 2], strides=[2, 2], padding='VALID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n",
      "Populating the interactive namespace from numpy and matplotlib\n",
      "[[array([[[238,  12,  34],\n",
      "        [238,  12,  30],\n",
      "        [230,  37,  25],\n",
      "        ...,\n",
      "        [249,  28,   7],\n",
      "        [240,  10,  23],\n",
      "        [234,  34,  19]],\n",
      "\n",
      "       [[236,  28,  29],\n",
      "        [233,  27,  17],\n",
      "        [234,  38,  24],\n",
      "        ...,\n",
      "        [230,   9,   9],\n",
      "        [235,  18,   9],\n",
      "        [230,  14,  29]],\n",
      "\n",
      "       [[251,  30,  30],\n",
      "        [235,  20,  13],\n",
      "        [247,  27,  34],\n",
      "        ...,\n",
      "        [248,  25,  33],\n",
      "        [240,  10,   3],\n",
      "        [233,  23,   2]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[246,  14,  22],\n",
      "        [251,  31,  37],\n",
      "        [239,  15,  20],\n",
      "        ...,\n",
      "        [253,  23,  22],\n",
      "        [249,   6,  38],\n",
      "        [246,   4,  36]],\n",
      "\n",
      "       [[248,  13,  19],\n",
      "        [251,  21,  20],\n",
      "        [242,  27,   4],\n",
      "        ...,\n",
      "        [235,  32,   3],\n",
      "        [242,  13,  10],\n",
      "        [234,   6,  36]],\n",
      "\n",
      "       [[236,   1,  36],\n",
      "        [247,  37,  29],\n",
      "        [249,   2,  23],\n",
      "        ...,\n",
      "        [254,  13,  11],\n",
      "        [241,  17,  32],\n",
      "        [251,  30,   2]]], dtype=uint8), array([1, 0])], [array([[[ 20,  30, 230],\n",
      "        [  0,  25, 250],\n",
      "        [ 30,   5, 254],\n",
      "        ...,\n",
      "        [  0,  15, 253],\n",
      "        [ 35,  20, 248],\n",
      "        [ 34,  16, 248]],\n",
      "\n",
      "       [[ 29,   8, 253],\n",
      "        [ 30,  11, 254],\n",
      "        [ 31,  16, 254],\n",
      "        ...,\n",
      "        [ 19,  27, 243],\n",
      "        [  0,  13, 237],\n",
      "        [  7,  27, 233]],\n",
      "\n",
      "       [[ 12,   8, 230],\n",
      "        [  3,  11, 238],\n",
      "        [ 33,  19, 235],\n",
      "        ...,\n",
      "        [ 12,   2, 244],\n",
      "        [ 28,   6, 252],\n",
      "        [ 11,  13, 254]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[  0,   9, 231],\n",
      "        [ 11,  38, 254],\n",
      "        [  6,  24, 251],\n",
      "        ...,\n",
      "        [  4,   4, 236],\n",
      "        [  8,   2, 241],\n",
      "        [ 32,  11, 244]],\n",
      "\n",
      "       [[  7,  21, 239],\n",
      "        [ 25,  39, 238],\n",
      "        [ 32,  29, 244],\n",
      "        ...,\n",
      "        [ 13,  24, 238],\n",
      "        [ 14,  14, 239],\n",
      "        [ 38,   3, 245]],\n",
      "\n",
      "       [[  4,   2, 233],\n",
      "        [  5,  37, 252],\n",
      "        [ 25,  33, 242],\n",
      "        ...,\n",
      "        [ 31,  18, 251],\n",
      "        [ 15,  34, 245],\n",
      "        [ 18,  25, 249]]], dtype=uint8), array([0, 1])]]\n"
     ]
    }
   ],
   "source": [
    "% pylab\n",
    "% matplotlib inline \n",
    "def createRedTestImage():\n",
    "    image = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "    for x in range(300):\n",
    "        for y in range(300):\n",
    "            image[x, y] = [np.random.randint(230.0, 255.0), \n",
    "                           np.random.randint(0.0, 40.0),\n",
    "                           np.random.randint(0.0, 40.0)]\n",
    "    return image\n",
    "\n",
    "def createBlueTestImage():\n",
    "    image = np.zeros((300, 300, 3), dtype=np.uint8)\n",
    "    for x in range(300):\n",
    "        for y in range(300):\n",
    "            image[x, y] = [np.random.randint(0.0, 40.0), \n",
    "                           np.random.randint(0.0, 40.0),\n",
    "                           np.random.randint(230.0, 255.0)]\n",
    "    return image\n",
    "            \n",
    "def getTrainSet(number=1000):\n",
    "    training_set = [[0, 0] for _ in range(number)]\n",
    "    for i in range(number):\n",
    "        # Rot\n",
    "        if np.random.rand() >= 0.5:\n",
    "            training_set[i][0] = createRedTestImage()\n",
    "            training_set[i][1] = np.array([1, 0])\n",
    "        else:\n",
    "            training_set[i][0] = createBlueTestImage()\n",
    "            training_set[i][1] = np.array([0, 1])\n",
    "    return training_set\n",
    "\n",
    "print(getTrainSet(2))                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452393233110603"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup - \n",
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x):\n",
    "    \"\"\"Model function for CNN.\"\"\"\n",
    "    # Input Layer\n",
    "    # Reshape X to 4-D tensor: [batch_size, width, height, channels]\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 300, 300, 3])\n",
    "\n",
    "    # Convolutional Layer #1\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Input Tensor Shape: [batch_size, 300, 300, 3]\n",
    "    # Output Tensor Shape: [batch_size, 298, 298, 32]\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"VALID\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Convolutional Layer #2\n",
    "    # Computes 32 features using a 5x5 filter with ReLU activation.\n",
    "    # Input Tensor Shape: [batch_size, 298, 298, 32]\n",
    "    # Output Tensor Shape: [batch_size, 296, 296, 32]\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=32,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"VALID\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #1\n",
    "    # First max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 296, 296, 32]\n",
    "    # Output Tensor Shape: [batch_size, 148, 148, 32]\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #3\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 148, 148, 32]\n",
    "    # Output Tensor Shape: [batch_size, 146, 146, 64]\n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=pool1,\n",
    "        filters=64,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"VALID\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 146, 146, 64]\n",
    "    # Output Tensor Shape: [batch_size, 144, 144, 128]\n",
    "    conv4 = tf.layers.conv2d(\n",
    "        inputs=conv3,\n",
    "        filters=128,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"VALID\",\n",
    "        activation=tf.nn.relu)\n",
    "\n",
    "    # Pooling Layer #2\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 144, 144, 128]\n",
    "    # Output Tensor Shape: [batch_size, 72, 72, 128]\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv4, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    # Convolutional Layer #5\n",
    "    # Computes 64 features using a 5x5 filter.\n",
    "    # Padding is added to preserve width and height.\n",
    "    # Input Tensor Shape: [batch_size, 72, 72, 128]\n",
    "    # Output Tensor Shape: [batch_size, 70, 70, 256]\n",
    "    conv5 = tf.layers.conv2d(\n",
    "        inputs=pool2,\n",
    "        filters=256,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"VALID\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Input Tensor Shape: [batch_size, 70, 70, 256]\n",
    "    # Output Tensor Shape: [batch_size, 68, 68, 256]\n",
    "    conv6 = tf.layers.conv2d(\n",
    "        inputs=conv5,\n",
    "        filters=256,\n",
    "        kernel_size=[5, 5],\n",
    "        padding=\"VALID\",\n",
    "        activation=tf.nn.relu)\n",
    "    \n",
    "    # Pooling Layer #3\n",
    "    # Second max pooling layer with a 2x2 filter and stride of 2\n",
    "    # Input Tensor Shape: [batch_size, 68, 68, 256]\n",
    "    # Output Tensor Shape: [batch_size, 34, 34, 256]\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv6, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    # Flatten tensor into a batch of vectors\n",
    "    # Input Tensor Shape:  [batch_size, 34, 34, 256]\n",
    "    # Output Tensor Shape: [batch_size, 34*34*256]\n",
    "    pool3_flat = tf.reshape(pool3, [-1, 34 * 34 * 256])\n",
    "    \n",
    "    # Dense Layer\n",
    "    # Densely connected layer with 1024 neurons\n",
    "    # Input Tensor Shape: [batch_size, 34*34*256]\n",
    "    # Output Tensor Shape: [batch_size, 1024]\n",
    "    dense = tf.layers.dense(inputs=pool3_flat, units=1024, activation=tf.nn.relu)\n",
    "    \n",
    "    # Add dropout operation; 0.6 probability that element will be kept\n",
    "    dropout = tf.layers.dropout(\n",
    "                inputs=dense, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "  \n",
    "    # Logits layer\n",
    "    # Input Tensor Shape: [batch_size, 1024]\n",
    "    # Output Tensor Shape: [batch_size, 2]\n",
    "    logits = tf.layers.dense(inputs=dropout, units=2)\n",
    "    \n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    onehot_labels = tf.one_hot(indices=tf.cast(labels, tf.int32), depth=10)\n",
    "    loss = tf.losses.softmax_cross_entropy(\n",
    "        onehot_labels=onehot_labels, logits=logits)\n",
    "    \n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "    # Load training and eval data\n",
    "    # TODO: HIER DIE DATENSÄTZE LADEN\n",
    "    # train_data = ...\n",
    "    # train_labels = ...\n",
    "    # eval_data = ...\n",
    "    # eval_labels = ...\n",
    " \n",
    "    # Create the Estimator\n",
    "    cnn_classifier = tf.estimator.Estimator(model_fn=discriminator, model_dir='/tmp/mnist_convnet_model')\n",
    "\n",
    "    # Set up logging for predictions\n",
    "    # Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "    tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "    logging_hook = tf.train.LoggingTensorHook(\n",
    "        tensors=tensors_to_log, every_n_iter=50)\n",
    "    \n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": train_data},\n",
    "        y=train_labels,\n",
    "        batch_size=5,\n",
    "        num_epochs=None,\n",
    "        shuffle=True)\n",
    "    \n",
    "    cnn_classifier.train(\n",
    "        input_fn=train_input_fn,\n",
    "        steps=20000,\n",
    "        hooks=[logging_hook])\n",
    "    \n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "        x={\"x\": eval_data},\n",
    "        y=eval_labels,\n",
    "        num_epochs=1,\n",
    "        shuffle=False)\n",
    "    eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    print(eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(unused_argv):\n",
    "  # Load training and eval data\n",
    "  mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "  train_data = mnist.train.images  # Returns np.array\n",
    "  train_labels = np.asarray(mnist.train.labels, dtype=np.int32)\n",
    "  eval_data = mnist.test.images  # Returns np.array\n",
    "  eval_labels = np.asarray(mnist.test.labels, dtype=np.int32)\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "  # Train the model\n",
    "  train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=100,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "  mnist_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=20000,\n",
    "      hooks=[logging_hook])\n",
    "\n",
    "  # Evaluate the model and print results\n",
    "  eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": eval_data},\n",
    "      y=eval_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "  eval_results = mnist_classifier.evaluate(input_fn=eval_input_fn)\n",
    "  print(eval_results)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  tf.app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
